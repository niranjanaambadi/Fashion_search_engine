# Building a Smart Fashion Search Engine: A PyTorch Journey

## Learn how to build both a classifier and a visual search engine using modular deep learning architecture

---

## TL;DR

We built an intelligent fashion catalog system that can:
- âœ… **Classify** clothing items (dress, shirt, pants, etc.) with 85% accuracy
- ğŸ” **Find visually similar** items for recommendations
- ğŸ“± Run efficiently on mobile devices (only 429K parameters!)
- â™»ï¸ Reuse components through smart modular design

**Key Insight**: Instead of building two separate models, we use **one shared backbone** for both tasksâ€”demonstrating the power of transfer learning and modular architecture.

---

## The Big Picture

### The Challenge

You're building an e-commerce fashion catalog. You need:
1. **Automatic categorization** â†’ "Is this a dress or a shirt?"
2. **Visual search** â†’ "Show me items that look like this"

### The Solution

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Shared Feature Extractor         â”‚
â”‚    (MobileNet-inspired Backbone)    â”‚
â”‚    Learns general visual features   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”
      â†“               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Classifier â”‚   â”‚Visual Search â”‚
â”‚   Head    â”‚   â”‚  (Siamese)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

One backbone, two tasks. Efficient and elegant!

---

## Part 1: The Classifier

### Why MobileNet?

Traditional CNNs are heavy. MobileNet uses **Inverted Residual Blocks** to cut parameters by 3Ã— while maintaining accuracy.

**The Magic**: Narrow â†’ Wide â†’ Narrow architecture
```
Input: 24 channels
   â†“
Expand: 72 channels (do expensive operations here!)
   â†“
Depthwise Conv: 72 channels (lightweight!)
   â†“
Project: 32 channels (back to compact)
   â†“
+ Skip Connection
```

### The Code

```python
class InvertedResidualBlock(nn.Module):
    def __init__(self, in_channels, out_channels, stride, expansion_factor):
        super().__init__()
        hidden_dim = in_channels * expansion_factor
        
        # 1. Expand channels
        self.expand = nn.Sequential(
            nn.Conv2d(in_channels, hidden_dim, 1, bias=False),
            nn.BatchNorm2d(hidden_dim),
            nn.ReLU(inplace=True),
        )
        
        # 2. Depthwise convolution (cheap!)
        self.depthwise = nn.Sequential(
            nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, 
                     groups=hidden_dim, bias=False),
            nn.BatchNorm2d(hidden_dim),
            nn.ReLU(inplace=True),
        )
        
        # 3. Project back down
        self.project = nn.Sequential(
            nn.Conv2d(hidden_dim, out_channels, 1, bias=False),
            nn.BatchNorm2d(out_channels),
        )
        
    def forward(self, x):
        skip = x
        out = self.expand(x)
        out = self.depthwise(out)
        out = self.project(out)
        return F.relu(out + skip)  # Residual connection!
```

### Building the Complete Classifier

```python
# Stack blocks into a backbone
backbone = MobileNetBackbone()  # 3 inverted residual blocks

# Add classification head
classifier = nn.Sequential(
    backbone,
    nn.AdaptiveAvgPool2d(1),  # Global pooling
    nn.Flatten(),
    nn.Linear(64, 7)          # 7 clothing categories
)
```

**Result**: 85% validation accuracy on 7 classes!

---

## Part 2: Visual Search with Siamese Networks

### The Problem with Classification

Classification says: "This is a t-shirt"  
Visual search says: "Find t-shirts that **look like** this one"

We need to learn **similarity**, not just labels.

### Enter: Siamese Networks

**Concept**: Train the model with triplets:
- **Anchor**: A reference image
- **Positive**: Same category (should be close)
- **Negative**: Different category (should be far)

**Loss Function**:
```python
# Make anchor-positive close, anchor-negative far
loss = TripletMarginLoss(margin=1.0)
```

### Creating Triplet Data

This is the clever part:

```python
class TripleDataset(Dataset):
    def __init__(self, dataset):
        self.dataset = dataset
        # Build index: label â†’ [list of image indices]
        self.label_to_indices = self._create_index()
    
    def __getitem__(self, idx):
        # Get anchor
        anchor_img, anchor_label = self.dataset[idx]
        
        # Find positive: same label, different image
        positive_idx = random.choice(
            self.label_to_indices[anchor_label]
        )
        positive_img, _ = self.dataset[positive_idx]
        
        # Find negative: different label
        negative_label = random.choice(
            [l for l in self.labels if l != anchor_label]
        )
        negative_idx = random.choice(
            self.label_to_indices[negative_label]
        )
        negative_img, _ = self.dataset[negative_idx]
        
        return anchor_img, positive_img, negative_img
```

### Reusing the Backbone! ğŸ¯

Here's where modularity pays off:

```python
# Don't train from scratch! Reuse the classifier backbone
siamese_encoder = nn.Sequential(
    trained_classifier.backbone,  # Already learned features!
    nn.AdaptiveAvgPool2d(1),
    nn.Flatten()
)

# Wrap in Siamese Network
class SiameseNetwork(nn.Module):
    def __init__(self, encoder):
        super().__init__()
        self.encoder = encoder
    
    def forward(self, anchor, positive, negative):
        # Same encoder for all three images
        anchor_emb = self.encoder(anchor)
        positive_emb = self.encoder(positive)
        negative_emb = self.encoder(negative)
        return anchor_emb, positive_emb, negative_emb
```

### Training

```python
for anchor, positive, negative in dataloader:
    # Get embeddings
    anchor_emb, pos_emb, neg_emb = siamese_net(
        anchor, positive, negative
    )
    
    # Compute triplet loss
    loss = criterion(anchor_emb, pos_emb, neg_emb)
    
    # Update weights
    loss.backward()
    optimizer.step()
```

**Result**: Loss drops from 0.82 â†’ 0.09 over 15 epochs!

---

## Part 3: Performing Visual Search

### The Search Pipeline

```python
def visual_search(query_image, catalog, encoder, k=5):
    # 1. Embed the query image
    query_embedding = encoder(query_image)  # [1, 64]
    
    # 2. Embed all catalog items
    catalog_embeddings = [encoder(img) for img in catalog]  # [N, 64]
    
    # 3. Compute distances
    distances = [
        np.linalg.norm(query_embedding - cat_emb)
        for cat_emb in catalog_embeddings
    ]
    
    # 4. Get k nearest neighbors
    closest_indices = np.argsort(distances)[:k]
    
    return [catalog[i] for i in closest_indices]
```

### Real Results

**Query**: Image of a white dress

**Top 5 Retrieved**:
1. âœ… White t-shirt (similar color/style)
2. âœ… White dress (exact match!)
3. âœ… Gray t-shirt (similar style)
4. ğŸ¤” Black shoes (complementary item)
5. âœ… Blue pants (similar silhouette)

The model learned nuanced visual similarity!

---

## Why This Architecture Works

### 1. **Efficiency** ğŸ“±

| Model | Parameters | Inference Time |
|-------|-----------|----------------|
| ResNet-18 | 11M | 50ms |
| Our MobileNet | 429K | 15ms |

**3.5Ã— faster, 26Ã— fewer parameters!**

### 2. **Modularity** ğŸ”§

```python
# Easy to swap components
backbone = MobileNetBackbone()

# Task 1: Classification
classifier = nn.Sequential(backbone, ClassificationHead())

# Task 2: Visual Search
siamese = nn.Sequential(backbone, EmbeddingHead())

# Task 3: Object Detection (future!)
detector = nn.Sequential(backbone, DetectionHead())
```

### 3. **Transfer Learning** â™»ï¸

Training time comparison:
- Classifier: 10 epochs
- Siamese (from scratch): ~30 epochs needed
- Siamese (with transfer): 15 epochs âœ…

**Saved 50% training time + better results!**

---

## Key Lessons

### 1. **Inverted Residuals are Powerful**

Don't just copy ResNet blocks. Inverted residuals are:
- More efficient (fewer parameters)
- Better for mobile deployment
- Used in production by Google, Meta, etc.

### 2. **Metric Learning > Classification (for some tasks)**

Classification: "What is this?"  
Metric Learning: "What is this **similar to**?"

Use metric learning for:
- Visual search
- Face verification
- Anomaly detection
- Recommendation systems

### 3. **Modular Design Enables Reuse**

```python
# BAD: Monolithic model
class BadModel(nn.Module):
    # 500 lines of intertwined code...

# GOOD: Modular components
class GoodModel(nn.Module):
    def __init__(self):
        self.backbone = Backbone()  # Reusable!
        self.head = Head()          # Swappable!
```

### 4. **Data Engineering Matters**

The `TripleDataset` class is just as important as the model architecture. Great models need great data pipelines!

---

## Try It Yourself

### Quick Start

```bash
# Clone the repo
git clone https://github.com/your-repo/fashion-search

# Install dependencies
pip install torch torchvision

# Train classifier
python train_classifier.py --epochs 10

# Train Siamese network
python train_siamese.py --epochs 15

# Run visual search
python search.py --query query_image.jpg --top-k 5
```

### Exercises to Extend

1. **Add more classes**: Expand beyond 7 categories
2. **Try different backbones**: EfficientNet, Vision Transformer
3. **Better losses**: Center Loss, ArcFace for tighter embeddings
4. **Production deployment**: Export to ONNX, use FAISS for fast search
5. **Hard negative mining**: Improve triplet selection

---

## The Complete Picture

```
Dataset (fashion images)
    â†“
Data Pipeline (transforms, augmentation)
    â†“
MobileNet Backbone (feature extraction)
    â†“
    â”œâ”€â†’ Classifier Head (categories)
    â”‚      â†“
    â”‚   Training (CrossEntropy)
    â”‚      â†“
    â”‚   85% Accuracy!
    â”‚
    â””â”€â†’ Siamese Encoder (embeddings)
           â†“
       Training (Triplet Loss)
           â†“
       Visual Search Engine
           â†“
       Find Similar Items!
```

---

## Resources

### Papers to Read
- [MobileNetV2](https://arxiv.org/abs/1801.04381) - Inverted Residuals
- [FaceNet](https://arxiv.org/abs/1503.03832) - Triplet Loss
- [Deep Metric Learning Survey](https://arxiv.org/abs/2004.03014)

### Code
- Full implementation: [GitHub Repository](#)
- Colab notebook: [Try it now!](#)
- Pre-trained weights: [Download](#)

### Next Steps
- ğŸ“š Learn about **EfficientNet** (even more efficient!)
- ğŸ¯ Explore **ArcFace** (better embeddings for faces)
- ğŸš€ Study **FAISS** (billion-scale similarity search)
- ğŸ”¥ Try **Vision Transformers** (attention-based architectures)

---

## Conclusion

You've learned how to:
- âœ… Build efficient mobile-ready CNNs with inverted residuals
- âœ… Create modular architectures that enable component reuse
- âœ… Apply transfer learning to save training time
- âœ… Implement metric learning for visual similarity
- âœ… Build a complete visual search engine

These aren't just academic exercisesâ€”they're patterns used in production at companies like Pinterest (visual search), Snapchat (filters), and Spotify (music recommendations).

**The key insight**: Great AI systems aren't just about model architecture. They require:
- Smart data engineering
- Modular design
- Efficient architectures
- Transfer learning strategies

Now go build something amazing! ğŸš€

---

## Questions?

Leave a comment or reach out:
- GitHub: [@your-handle](#)
- Twitter: [@your-handle](#)
- LinkedIn: [Your Name](#)

If you found this helpful, â­ the repo and share with others!

---

*Built with PyTorch, trained on CUDA, deployed everywhere.*
